{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302e5323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='2,3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='3'\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c17eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ci_with_y'shape:  (31345, 64001)\n",
      "hi_with_y'shape:  (39791, 64001)\n",
      "X shape:  (71136, 64000) Y shape:  (71136,)\n",
      "train_X'shape:  (53300, 64000)\n",
      "val_X'shape:  (17700, 64000)\n"
     ]
    }
   ],
   "source": [
    "train_ci_ids = [\"HM0001\", \"HM0002\", \"HS0003\", \"HS0004\", \"HS0005\", \"HS0006\", \"HS0011\"]\n",
    "# test_ci_ids = [\"HM0004\", \"HM0005\", \"HS0001\", \"HS0002\", \"HS0009\", \"HS0010\", \"HS0012\",\"HS0013\"]\n",
    "\n",
    "train_hi_ids = [\"HL0052\", \"HL0074\", \"HL0028\", \"HL0150\", \"HL0039\", \"HL0019\", \"HL0029\"]\n",
    "# test_hi_ids = [\"HL0005\", \"HL0075\", \"HL0024\", \"HL0034\", \"HL0089\"]\n",
    "\n",
    "\n",
    "def loads_npy_with_y(base_path, ids, y_value):\n",
    "    loaded_npa_list = []\n",
    "    for id in ids:\n",
    "        file_path = glob.glob(f'{base_path}/{id}*.npy')[0]\n",
    "        if not isinstance(file_path, str):\n",
    "            raise Exception(f\"could'n find id({id}) file \")\n",
    "        npa = np.load(file_path)\n",
    "        n = npa.shape[0]\n",
    "        y = np.array([y_value] * n, dtype=npa.dtype)\n",
    "        npa = np.concatenate([npa,y.reshape(-1,1)], axis=1)\n",
    "        loaded_npa_list.append(npa)\n",
    "    \n",
    "    return np.concatenate(loaded_npa_list, axis=0)\n",
    "\n",
    "    \n",
    "ci_with_y = loads_npy_with_y('CI', train_ci_ids, 1)\n",
    "print(\"ci_with_y'shape: \", ci_with_y.shape)\n",
    "\n",
    "hi_with_y = loads_npy_with_y('HI', train_hi_ids, 0)\n",
    "print(\"hi_with_y'shape: \", hi_with_y.shape)\n",
    "\n",
    "x_with_y = np.concatenate([ci_with_y, hi_with_y], axis=0)\n",
    "np.random.shuffle(x_with_y)\n",
    "\n",
    "\n",
    "X = x_with_y[:,:-1]\n",
    "Y = x_with_y[:,-1]\n",
    "print(\"X shape: \", X.shape, \"Y shape: \", Y.shape)\n",
    "\n",
    "\n",
    "validation_rate = 0.25\n",
    "h = X.shape[0]\n",
    "val_num = int(h * validation_rate)\n",
    "train_num = h - val_num\n",
    "\n",
    "train_num = (train_num//100)*100\n",
    "val_num = (val_num//100)*100\n",
    "\n",
    "train_X = X[:train_num]\n",
    "train_Y = Y[:train_num]\n",
    "print(\"train_X'shape: \", train_X.shape)\n",
    "val_X = X[-val_num:]\n",
    "val_Y = Y[-val_num:]\n",
    "print(\"val_X'shape: \", val_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80381a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model:int, num_heads:int, batch_size:int):\n",
    "        super(MHAttention,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.batch_size = batch_size\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "\n",
    "\n",
    "        self.sqrt_depth = tf.cast(self.depth, dtype=tf.float32)\n",
    "\n",
    "        self.query = tf.keras.layers.Dense(self.d_model)\n",
    "        self.key = tf.keras.layers.Dense(self.d_model)\n",
    "        self.value = tf.keras.layers.Dense(self.d_model)\n",
    "        self.outweight = tf.keras.layers.Dense(self.d_model)\n",
    "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "\n",
    "\n",
    "    def split_heads(self, input):\n",
    "        # if self.batch_size == None:\n",
    "        #     batch_size = tf.shape(input)[0]\n",
    "        # else: \n",
    "        #     batch_size = self.batch_size\n",
    "        input = tf.reshape(input,(self.batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(input, perm=[0,2,1,3])\n",
    "\n",
    "\n",
    "    def __call__(self, input):\n",
    "        # if self.batch_size == None:\n",
    "        #     batch_size = tf.shape(input)[0]\n",
    "        # else: \n",
    "        #     batch_size = self.batch_size\n",
    "        query = self.query(input)\n",
    "        key = self.key(input)\n",
    "        value = self.value(input)\n",
    "\n",
    "        query_splitted = self.split_heads(query)\n",
    "        key_splitted = self.split_heads(key)\n",
    "        value_splitted = self.split_heads(value)\n",
    "\n",
    "        q_mat_k = tf.matmul(query_splitted, key_splitted, transpose_b=True)\n",
    "        q_mat_k = q_mat_k / self.sqrt_depth\n",
    "\n",
    "        q_mat_k_soft = self.softmax(q_mat_k)\n",
    "        attention_score = tf.matmul(q_mat_k_soft, value_splitted)\n",
    "        attention_score = tf.transpose(attention_score, perm=[0,2,1,3])\n",
    "        attention_score = tf.reshape(attention_score, (self.batch_size, -1, self.d_model))\n",
    "\n",
    "        return self.outweight(attention_score)\n",
    "\n",
    "\n",
    "class MyEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model:int, num_heads:int, batch_size:int):\n",
    "        super(MyEncoder,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.batch_size = batch_size\n",
    "        self.multi_head_attention = MHAttention(self.d_model, self.num_heads, self.batch_size)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(d_model)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.act1 = tf.keras.layers.Activation('relu')\n",
    "\n",
    "    def __call__(self, input):\n",
    "        a = self.multi_head_attention(input)\n",
    "        con = tf.concat([input,a], axis=-1)\n",
    "        o1 = self.dense1(con)\n",
    "        o1 = self.layer_norm(o1)\n",
    "        o1 = self.act1(o1)       \n",
    "\n",
    "        return o1\n",
    "\n",
    "class Res1(tf.keras.layers.Layer):\n",
    "# class Res1(tf.keras.Model):\n",
    "    def __init__(self,filters:int,kernel_size:int,padding:str,activation:str, flag_res:bool=True):\n",
    "        super(Res1,self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.FLAG_RES = flag_res\n",
    "\n",
    "        self.conv1 = Conv1D(self.filters, kernel_size=self.kernel_size, padding=self.padding)\n",
    "        self.batch1 = BatchNormalization()\n",
    "        self.act1 = Activation(self.activation)\n",
    "\n",
    "        self.conv2 = Conv1D(self.filters, kernel_size=self.kernel_size, padding=self.padding)\n",
    "        self.batch2 = BatchNormalization()\n",
    "        self.act2 = Activation(self.activation)\n",
    "\n",
    "        self.conv3 = Conv1D(self.filters, kernel_size=self.kernel_size, padding=self.padding)\n",
    "        self.batch3 = BatchNormalization()\n",
    "        self.act3 = Activation(self.activation)\n",
    "\n",
    "        self.pool = MaxPool1D(strides=2)\n",
    "\n",
    "\n",
    "    def __call__(self, input):\n",
    "        x1 = self.conv1(input)\n",
    "        x2 = self.batch1(x1)\n",
    "        x3 = self.act1(x2)\n",
    "\n",
    "        x4 = self.conv2(x3)\n",
    "        x5 = self.batch2(x4)\n",
    "        x6 = self.act2(x5)\n",
    "\n",
    "        x7 = self.conv3(x6)\n",
    "        x8 = self.batch3(x7)\n",
    "        x9 = self.act3(x8)\n",
    "\n",
    "        if self.FLAG_RES:\n",
    "            x_added = tf.add(x9, x3)\n",
    "            return self.pool(x_added)\n",
    "        else :\n",
    "            return self.pool(x9)\n",
    "\n",
    "\n",
    "        \n",
    "class MobileDense1(tf.keras.layers.Layer):\n",
    "    ## the number of filters must be twice of input channels because the output will be concatenated. \n",
    "    def __init__(self,filters:int, kernel_size:int=3, padding:str='same', activation:str='relu', depth_mul:int=4):\n",
    "        super(MobileDense1,self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.depth_mul = depth_mul\n",
    "\n",
    "        if self.filters & 1:\n",
    "            raise Exception(\"from MobileDense1: the parameter named 'filters' must be even number\")\n",
    "\n",
    "        self.conv1 = DepthwiseConv2D(self.kernel_size, depth_multiplier=self.depth_mul, padding=self.padding)\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.act1 = Activation(self.activation)\n",
    "        self.conv2 = Conv2D(self.filters//2, 1, padding=self.padding)\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.act2 = Activation(self.activation)\n",
    "        self.pool = MaxPool2D(strides=2)\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        if input.shape[-1] != self.filters//2:\n",
    "            raise Exception(\"from MobileDense1: the input channels must be half of the 'filters' parameter\")\n",
    "\n",
    "        x1 = self.conv1(input)\n",
    "        x2 = self.bn1(x1)\n",
    "        x3 = self.act1(x2)\n",
    "        x4 = self.conv2(x3)\n",
    "        x5 = self.bn2(x4)\n",
    "        x6 = self.act2(x5)\n",
    "        x7 = tf.concat([input, x6],axis=-1)\n",
    "        \n",
    "        return self.pool(x7)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d57928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model1():\n",
    "    ## basic CNN\n",
    "    ACTIVATION_FN = 'elu'\n",
    "    filters = 32\n",
    "\n",
    "    input_layer = Input(shape=(64000,1))\n",
    "    conv1 = Conv1D(filters, kernel_size=3, padding='same')(input_layer)\n",
    "    batch1 = BatchNormalization()(conv1)\n",
    "    act1 = Activation(ACTIVATION_FN)(batch1)\n",
    "    pool1 = MaxPool1D(strides=2)(act1)\n",
    "\n",
    "    ## 32000, 64\n",
    "    conv2 = Conv1D(filters *2, kernel_size=3, padding='same')(pool1)\n",
    "    batch2 = BatchNormalization()(conv2)\n",
    "    act2 = Activation(ACTIVATION_FN)(batch2)\n",
    "    pool2 = MaxPool1D(strides=2)(act2)\n",
    "\n",
    "    ## 16000, 128\n",
    "    conv3 = Conv1D(filters *4, kernel_size=3, padding='same')(pool2)\n",
    "    batch3 = BatchNormalization()(conv3)\n",
    "    act3 = Activation(ACTIVATION_FN)(batch3)\n",
    "    pool3 = MaxPool1D(strides=2)(act3)\n",
    "\n",
    "    ## 8000, 256\n",
    "    conv4 = Conv1D(filters*8, kernel_size=3, padding='same')(pool3)\n",
    "    batch4 = BatchNormalization()(conv4)\n",
    "    act4 = Activation(ACTIVATION_FN)(batch4)\n",
    "    pool4 = MaxPool1D(strides=2)(act4)\n",
    "\n",
    "    ## 4000, 512\n",
    "    conv5 = Conv1D(filters *16, kernel_size=3, padding='same')(pool4)\n",
    "    batch5 = BatchNormalization()(conv5)\n",
    "    act5 = Activation(ACTIVATION_FN)(batch5)\n",
    "    pool5 = MaxPool1D(strides=2)(act5)\n",
    "\n",
    "    ## 2000, 512\n",
    "    conv6 = Conv1D(filters *16, kernel_size=3, padding='same')(pool5)\n",
    "    batch6 = BatchNormalization()(conv6)\n",
    "    flat1 = Flatten()(batch6)\n",
    "    dense1 = Dense(200, activation='relu')(flat1)\n",
    "    batch7 = BatchNormalization()(dense1)\n",
    "    dense2 = Dense(20, activation='relu')(batch7)\n",
    "    # dense2 = Dense(20, activation='relu')(dense1)\n",
    "    output_layer = Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d168bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model2(batch_size):\n",
    "    ## resnet only\n",
    "    BATCH_SIZE = batch_size\n",
    "    ACTIVATION_FN = 'elu'\n",
    "    filters = 32\n",
    "\n",
    "\n",
    "    input_layer = Input(shape=(64000,1), batch_size=BATCH_SIZE)\n",
    "\n",
    "    res1 = Res1(filters, 3, 'same', ACTIVATION_FN)(input_layer)\n",
    "    ## 32000, 32\n",
    "\n",
    "    res2 = Res1(filters*2, 3, 'same', ACTIVATION_FN)(res1)\n",
    "    ## 16000, 64\n",
    "\n",
    "    res3 = Res1(filters*4, 3, 'same', ACTIVATION_FN)(res2)\n",
    "    ## 8000, 128\n",
    "\n",
    "    res4 = Res1(filters*8, 3, 'same', ACTIVATION_FN)(res3)\n",
    "    ## 4000, 256\n",
    "\n",
    "    res5 = Res1(filters*16, 3, 'same', ACTIVATION_FN)(res4)\n",
    "    ## 2000, 512\n",
    "\n",
    "    res6 = Res1(filters*16, 3, 'same', ACTIVATION_FN)(res5)\n",
    "    ## 1000, 512\n",
    "    \n",
    "    res7 = Res1(filters*16, 3, 'same', ACTIVATION_FN)(res6)\n",
    "    ## 500, 512\n",
    "    \n",
    "    flat5 = Flatten()(res7)\n",
    "#     flat5 = Flatten()(res1)\n",
    "    dense5 = Dense(200, activation='relu')(flat5)\n",
    "    batch5 = BatchNormalization()(dense5)\n",
    "    dense6 = Dense(20, activation='relu')(batch5)\n",
    "    output_layer = Dense(1, activation='sigmoid')(dense6)    \n",
    "    \n",
    "    return Model(input_layer, output_layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676f6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model3(batch_size):\n",
    "    ## CNN + ATTENTION\n",
    "    ACTIVATION_FN = 'elu'\n",
    "    filters = 32\n",
    "\n",
    "    input_layer = Input(shape=(64000,1))\n",
    "    conv1 = Conv1D(filters, kernel_size=3, padding='same')(input_layer)\n",
    "    batch1 = BatchNormalization()(conv1)\n",
    "    act1 = Activation(ACTIVATION_FN)(batch1)\n",
    "    pool1 = MaxPool1D(strides=2)(act1)\n",
    "\n",
    "    ## 32000, 64\n",
    "    conv2 = Conv1D(filters *2, kernel_size=3, padding='same')(pool1)\n",
    "    batch2 = BatchNormalization()(conv2)\n",
    "    act2 = Activation(ACTIVATION_FN)(batch2)\n",
    "    pool2 = MaxPool1D(strides=2)(act2)\n",
    "\n",
    "    ## 16000, 128\n",
    "    conv3 = Conv1D(filters *4, kernel_size=3, padding='same')(pool2)\n",
    "    batch3 = BatchNormalization()(conv3)\n",
    "    act3 = Activation(ACTIVATION_FN)(batch3)\n",
    "    pool3 = MaxPool1D(strides=2)(act3)\n",
    "\n",
    "    ## 8000, 256\n",
    "    conv4 = Conv1D(filters*8, kernel_size=3, padding='same')(pool3)\n",
    "    batch4 = BatchNormalization()(conv4)\n",
    "    act4 = Activation(ACTIVATION_FN)(batch4)\n",
    "    pool4 = MaxPool1D(strides=2)(act4)\n",
    "\n",
    "    ## 4000, 512\n",
    "    conv5 = Conv1D(filters *16, kernel_size=3, padding='same')(pool4)\n",
    "    batch5 = BatchNormalization()(conv5)\n",
    "    act5 = Activation(ACTIVATION_FN)(batch5)\n",
    "    pool5 = MaxPool1D(strides=2)(act5)\n",
    "\n",
    "    ## 2000, 512\n",
    "    conv6 = Conv1D(filters *16, kernel_size=3, padding='same')(pool5)\n",
    "    tr1 = tf.transpose(conv6, perm=[0,2,1])\n",
    "    enc1 = MyEncoder(2000,8,BATCH_SIZE)(tr1)\n",
    "    \n",
    "    \n",
    "    flat1 = Flatten()(enc1)\n",
    "    dense1 = Dense(200, activation='relu')(flat1)\n",
    "    batch7 = BatchNormalization()(dense1)\n",
    "    dense2 = Dense(20, activation='relu')(batch7)\n",
    "    # dense2 = Dense(20, activation='relu')(dense1)\n",
    "    output_layer = Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "#     conv1 = Conv1D(filters *16, kernel_size=3, padding='same')(res5)\n",
    "#     ## 2000, 512\n",
    "#     tr1 = tf.transpose(conv1,perm=[0,2,1])\n",
    "#     # print(\"input_layer, \", res10)\n",
    "#     enc1 = MyEncoder(2000,8,BATCH_SIZE)(tr1)\n",
    "\n",
    "#     flat1 = Flatten()(enc1)\n",
    "#     batch6 = BatchNormalization()(flat1)\n",
    "#     dense1 = Dense(200, activation='relu')(batch6)\n",
    "#     batch7 = BatchNormalization()(dense1)\n",
    "#     dense2 = Dense(20, activation='relu')(batch7)\n",
    "#     # dense2 = Dense(20, activation='relu')(dense1)\n",
    "#     output_layer = Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "#     model = Model(input_layer, output_layer)\n",
    "    \n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ffe7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model4(batch_size):\n",
    "    ## RESNET + ATTENTION\n",
    "    BATCH_SIZE = batch_size\n",
    "    ACTIVATION_FN = 'elu'\n",
    "    filters = 32\n",
    "\n",
    "\n",
    "    input_layer = Input(shape=(64000,1), batch_size=BATCH_SIZE)\n",
    "\n",
    "    res1 = Res1(filters, 3, 'same', ACTIVATION_FN)(input_layer)\n",
    "    ## 32000, 32\n",
    "\n",
    "    res2 = Res1(filters*2, 3, 'same', ACTIVATION_FN)(res1)\n",
    "    ## 16000, 64\n",
    "\n",
    "    res3 = Res1(filters*4, 3, 'same', ACTIVATION_FN)(res2)\n",
    "    ## 8000, 128\n",
    "\n",
    "    res4 = Res1(filters*8, 3, 'same', ACTIVATION_FN)(res3)\n",
    "    ## 4000, 256\n",
    "\n",
    "    res5 = Res1(filters*16, 3, 'same', ACTIVATION_FN)(res4)\n",
    "    ## 2000, 512\n",
    "\n",
    "    conv1 = Conv1D(filters *16, kernel_size=3, padding='same')(res5)\n",
    "    ## 2000, 512\n",
    "    tr1 = tf.transpose(conv1,perm=[0,2,1])\n",
    "    # print(\"input_layer, \", res10)\n",
    "    enc1 = MyEncoder(2000,8,BATCH_SIZE)(tr1)\n",
    "\n",
    "    flat1 = Flatten()(enc1)\n",
    "    batch6 = BatchNormalization()(flat1)\n",
    "    dense1 = Dense(200, activation='relu')(batch6)\n",
    "    batch7 = BatchNormalization()(dense1)\n",
    "    dense2 = Dense(20, activation='relu')(batch7)\n",
    "    # dense2 = Dense(20, activation='relu')(dense1)\n",
    "    output_layer = Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c6ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model5(batch_size):\n",
    "    ## mobile + dense + att\n",
    "    BATCH_SIZE = batch_size\n",
    "    ACTIVATION_FN = 'elu'\n",
    "    filters = 32\n",
    "    \n",
    "    input_layer = Input(shape=(64000,1), batch_size=BATCH_SIZE)\n",
    "    conv = Conv1D(320,3, padding='same')(input_layer)\n",
    "    bn = BatchNormalization()(conv)\n",
    "    act = Activation('relu')(bn)\n",
    "    reshaped1 = tf.reshape(act, (-1,800,800,32))\n",
    "    md1 = MobileDense1(64, activation=ACTIVATION_FN)(reshaped1)\n",
    "    ## 400, 64\n",
    "\n",
    "    md2 = MobileDense1(128, activation=ACTIVATION_FN)(md1)\n",
    "    ## 200, 128\n",
    "\n",
    "    md3 = MobileDense1(256, activation=ACTIVATION_FN)(md2)\n",
    "    ## 100, 256\n",
    "\n",
    "    md4 = MobileDense1(512, activation=ACTIVATION_FN)(md3)\n",
    "    ## 50, 512\n",
    "\n",
    "    reshaped2 = tf.reshape(md4,(-1,2500,512))\n",
    "    tr1 = tf.transpose(reshaped2,[0,2,1])\n",
    "    enc1 = MyEncoder(2500,10,BATCH_SIZE)(tr1)\n",
    "    enc2 = MyEncoder(2500,10,BATCH_SIZE)(enc1)\n",
    "\n",
    "    flat1 = Flatten()(enc2)\n",
    "    batch6 = BatchNormalization()(flat1)\n",
    "    dense1 = Dense(200, activation='relu')(batch6)\n",
    "    batch7 = BatchNormalization()(dense1)\n",
    "    dense2 = Dense(20, activation='relu')(batch7)\n",
    "    # dense2 = Dense(20, activation='relu')(dense1)\n",
    "    output_layer = Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1f1a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(10, 64000, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (10, 64000, 32)      128         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (10, 64000, 32)      128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (10, 64000, 32)      0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (10, 64000, 32)      3104        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (10, 64000, 32)      128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (10, 64000, 32)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (10, 64000, 32)      3104        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (10, 64000, 32)      128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (10, 64000, 32)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add (TFOpLambda)        (10, 64000, 32)      0           activation_2[0][0]               \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (10, 32000, 32)      0           tf.math.add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (10, 32000, 64)      6208        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (10, 32000, 64)      256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (10, 32000, 64)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (10, 32000, 64)      12352       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (10, 32000, 64)      256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (10, 32000, 64)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (10, 32000, 64)      12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (10, 32000, 64)      256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (10, 32000, 64)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_1 (TFOpLambda)      (10, 32000, 64)      0           activation_5[0][0]               \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (10, 16000, 64)      0           tf.math.add_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (10, 16000, 128)     24704       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (10, 16000, 128)     512         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (10, 16000, 128)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (10, 16000, 128)     49280       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (10, 16000, 128)     512         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (10, 16000, 128)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (10, 16000, 128)     49280       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (10, 16000, 128)     512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (10, 16000, 128)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_2 (TFOpLambda)      (10, 16000, 128)     0           activation_8[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (10, 8000, 128)      0           tf.math.add_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (10, 8000, 256)      98560       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (10, 8000, 256)      1024        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (10, 8000, 256)      0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (10, 8000, 256)      196864      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (10, 8000, 256)      1024        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (10, 8000, 256)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (10, 8000, 256)      196864      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (10, 8000, 256)      1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (10, 8000, 256)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_3 (TFOpLambda)      (10, 8000, 256)      0           activation_11[0][0]              \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (10, 4000, 256)      0           tf.math.add_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (10, 4000, 512)      393728      max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (10, 4000, 512)      2048        conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (10, 4000, 512)      0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (10, 4000, 512)      786944      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (10, 4000, 512)      2048        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (10, 4000, 512)      0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (10, 4000, 512)      786944      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (10, 4000, 512)      2048        conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (10, 4000, 512)      0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_4 (TFOpLambda)      (10, 4000, 512)      0           activation_14[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (10, 2000, 512)      0           tf.math.add_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (10, 2000, 512)      786944      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose (TFOpLam (10, 512, 2000)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (10, 512, 2000)      4002000     tf.compat.v1.transpose[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (10, 512, 2000)      4002000     tf.compat.v1.transpose[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape (TFOpLambda)         (10, 512, 8, 250)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_1 (TFOpLambda)       (10, 512, 8, 250)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_1 (TFOpL (10, 8, 512, 250)    0           tf.reshape[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_2 (TFOpL (10, 8, 512, 250)    0           tf.reshape_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.linalg.matmul (TFOpLambda)   (10, 8, 512, 512)    0           tf.compat.v1.transpose_1[0][0]   \n",
      "                                                                 tf.compat.v1.transpose_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (10, 512, 2000)      4002000     tf.compat.v1.transpose[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda)    (10, 8, 512, 512)    0           tf.linalg.matmul[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_2 (TFOpLambda)       (10, 512, 8, 250)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (10, 8, 512, 512)    0           tf.math.truediv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_3 (TFOpL (10, 8, 512, 250)    0           tf.reshape_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.linalg.matmul_1 (TFOpLambda) (10, 8, 512, 250)    0           softmax[0][0]                    \n",
      "                                                                 tf.compat.v1.transpose_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_4 (TFOpL (10, 512, 8, 250)    0           tf.linalg.matmul_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_3 (TFOpLambda)       (10, 512, 2000)      0           tf.compat.v1.transpose_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (10, 512, 2000)      4002000     tf.reshape_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (10, 512, 4000)      0           tf.compat.v1.transpose[0][0]     \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (10, 512, 2000)      8002000     tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (10, 512, 2000)      4000        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (10, 512, 2000)      0           layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (10, 1024000)        0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (10, 1024000)        4096000     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (10, 200)            204800200   batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (10, 200)            800         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (10, 20)             4020        batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (10, 1)              21          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 236,334,305\n",
      "Trainable params: 234,279,953\n",
      "Non-trainable params: 2,054,352\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 10\n",
    "MODEL_NUM = 4\n",
    "\n",
    "def make_model_with_num(MODEL_NUM, BATCH_SIZE):\n",
    "    if MODEL_NUM == 1:\n",
    "        model = make_model1()\n",
    "    elif MODEL_NUM == 2:\n",
    "        model = make_model2(BATCH_SIZE)\n",
    "    elif MODEL_NUM == 3:\n",
    "        model = make_model3(BATCH_SIZE)\n",
    "    elif MODEL_NUM == 4:\n",
    "        model = make_model4(BATCH_SIZE)\n",
    "    elif MODEL_NUM == 5:\n",
    "        model = make_model5(BATCH_SIZE)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = make_model_with_num(MODEL_NUM,BATCH_SIZE)\n",
    "model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0e81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_with_small_dataset(train_X, train_Y, val_X, val_Y, BATCH_SIZE, EPOCHS):\n",
    "\n",
    "#     LIMIT_N = train_X.shape[0]\n",
    "    SPLIT_N = 11\n",
    "    devided_train_index = list(map(int,np.linspace(0,train_X.shape[0],SPLIT_N)))\n",
    "    devided_val_index = list(map(int,np.linspace(0,val_X.shape[0],SPLIT_N)))\n",
    "    \n",
    "    print('devided_train_index: ',devided_train_index)\n",
    "    print('devided_val_index: ',devided_val_index)\n",
    "    \n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    accs = []\n",
    "    val_accs = []\n",
    "    for i in range(SPLIT_N-1):\n",
    "#     i=0\n",
    "#         start_num = devided_index[i]\n",
    "#     start_num = 0\n",
    "#         end_num = devided_index[i+1]\n",
    "#     end_num = 2000\n",
    "        print(f\"sub_dataset:{i}\")\n",
    "        history = model.fit(train_X[ devided_train_index[i]:devided_train_index[i+1] ], train_Y[ devided_train_index[i]:devided_train_index[i+1] ], validation_data=(val_X[ devided_val_index[i]:devided_val_index[i+1] ], val_Y[ devided_val_index[i]:devided_val_index[i+1] ]), batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True, verbose=2)\n",
    "\n",
    "        accs.append(history.history['accuracy'])\n",
    "        val_accs.append(history.history['val_accuracy'])\n",
    "        losses.append(history.history['loss'])\n",
    "        val_losses.append(history.history['val_loss'])\n",
    "    \n",
    "    return {\"accuracy\": np.stack(accs), \"val_accuracy\": np.stack(val_accs), \"loss\": np.stack(losses), \"val_loss\": np.stack(val_losses)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5c56610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_all_at_once(train_X, train_Y, val_X, val_Y, BATCH_SIZE, EPOCHS):\n",
    "    history = model.fit(train_X,train_Y, validation_data=(val_X, val_Y), batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True, verbose=2)\n",
    "    \n",
    "    return history.history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11142c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/15\n",
    "# 10671/10671 - 535s - loss: 0.2005 - accuracy: 0.9194 - val_loss: 0.7918 - val_accuracy: 0.7620\n",
    "# Epoch 2/15\n",
    "# 10671/10671 - 506s - loss: 0.1128 - accuracy: 0.9578 - val_loss: 0.0663 - val_accuracy: 0.9755\n",
    "# Epoch 3/15\n",
    "# 10671/10671 - 506s - loss: 0.0616 - accuracy: 0.9772 - val_loss: 0.0163 - val_accuracy: 0.9961\n",
    "# Epoch 4/15\n",
    "# 10671/10671 - 506s - loss: 0.0412 - accuracy: 0.9861 - val_loss: 0.0125 - val_accuracy: 0.9963\n",
    "# Epoch 5/15\n",
    "# 10671/10671 - 506s - loss: 0.0305 - accuracy: 0.9897 - val_loss: 2.7504 - val_accuracy: 0.6195\n",
    "# Epoch 6/15\n",
    "# 10671/10671 - 506s - loss: 0.0232 - accuracy: 0.9925 - val_loss: 0.5461 - val_accuracy: 0.8438\n",
    "# Epoch 7/15\n",
    "# 10671/10671 - 506s - loss: 0.0215 - accuracy: 0.9928 - val_loss: 0.0109 - val_accuracy: 0.9967\n",
    "# Epoch 8/15\n",
    "# 10671/10671 - 506s - loss: 0.0188 - accuracy: 0.9938 - val_loss: 0.8797 - val_accuracy: 0.8350\n",
    "# Epoch 9/15\n",
    "# 10671/10671 - 506s - loss: 0.0298 - accuracy: 0.9900 - val_loss: 0.0506 - val_accuracy: 0.9806\n",
    "# Epoch 10/15\n",
    "# 10671/10671 - 506s - loss: 0.0206 - accuracy: 0.9938 - val_loss: 0.0183 - val_accuracy: 0.9945\n",
    "# Epoch 11/15\n",
    "# 10671/10671 - 506s - loss: 0.0120 - accuracy: 0.9962 - val_loss: 0.0437 - val_accuracy: 0.9839\n",
    "# Epoch 12/15\n",
    "# 10671/10671 - 506s - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0252 - val_accuracy: 0.9920\n",
    "# Epoch 13/15\n",
    "# 10671/10671 - 506s - loss: 0.0120 - accuracy: 0.9958 - val_loss: 0.0073 - val_accuracy: 0.9976\n",
    "# Epoch 14/15\n",
    "# 10671/10671 - 506s - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.1818 - val_accuracy: 0.9425\n",
    "# Epoch 15/15\n",
    "# 10671/10671 - 506s - loss: 0.0094 - accuracy: 0.9966 - val_loss: 0.0718 - val_accuracy: 0.9750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f17d6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devided_train_index:  [0, 5330, 10660, 15990, 21320, 26650, 31980, 37310, 42640, 47970, 53300]\n",
      "devided_val_index:  [0, 1770, 3540, 5310, 7080, 8850, 10620, 12390, 14160, 15930, 17700]\n",
      "sub_dataset:0\n",
      "Epoch 1/10\n",
      "533/533 - 146s - loss: 0.1567 - accuracy: 0.9422 - val_loss: 1.9359 - val_accuracy: 0.5271\n",
      "Epoch 2/10\n",
      "533/533 - 123s - loss: 0.0706 - accuracy: 0.9752 - val_loss: 0.0453 - val_accuracy: 0.9887\n",
      "Epoch 3/10\n",
      "533/533 - 123s - loss: 0.0528 - accuracy: 0.9841 - val_loss: 2.1119 - val_accuracy: 0.4881\n",
      "Epoch 4/10\n",
      "533/533 - 123s - loss: 0.0436 - accuracy: 0.9856 - val_loss: 1.3209 - val_accuracy: 0.6819\n",
      "Epoch 5/10\n",
      "533/533 - 123s - loss: 0.0226 - accuracy: 0.9929 - val_loss: 2.5735 - val_accuracy: 0.5429\n",
      "Epoch 6/10\n",
      "533/533 - 123s - loss: 0.0494 - accuracy: 0.9842 - val_loss: 3.6383 - val_accuracy: 0.5791\n",
      "Epoch 7/10\n",
      "533/533 - 123s - loss: 0.0452 - accuracy: 0.9850 - val_loss: 0.1352 - val_accuracy: 0.9599\n",
      "Epoch 8/10\n",
      "533/533 - 123s - loss: 0.0246 - accuracy: 0.9927 - val_loss: 0.0056 - val_accuracy: 0.9994\n",
      "Epoch 9/10\n",
      "533/533 - 123s - loss: 0.0251 - accuracy: 0.9925 - val_loss: 2.6498 - val_accuracy: 0.4503\n",
      "Epoch 10/10\n",
      "533/533 - 123s - loss: 0.0292 - accuracy: 0.9902 - val_loss: 0.0030 - val_accuracy: 0.9989\n",
      "sub_dataset:1\n",
      "Epoch 1/10\n",
      "533/533 - 123s - loss: 0.0414 - accuracy: 0.9848 - val_loss: 2.1515 - val_accuracy: 0.4960\n",
      "Epoch 2/10\n",
      "533/533 - 123s - loss: 0.0203 - accuracy: 0.9934 - val_loss: 1.8597 - val_accuracy: 0.6401\n",
      "Epoch 3/10\n",
      "533/533 - 123s - loss: 0.0314 - accuracy: 0.9901 - val_loss: 0.0537 - val_accuracy: 0.9797\n",
      "Epoch 4/10\n",
      "533/533 - 123s - loss: 0.0164 - accuracy: 0.9949 - val_loss: 0.2101 - val_accuracy: 0.9017\n",
      "Epoch 5/10\n",
      "533/533 - 123s - loss: 0.0369 - accuracy: 0.9878 - val_loss: 1.3192 - val_accuracy: 0.6672\n",
      "Epoch 6/10\n",
      "533/533 - 123s - loss: 0.0252 - accuracy: 0.9919 - val_loss: 1.5050 - val_accuracy: 0.6582\n",
      "Epoch 7/10\n",
      "533/533 - 123s - loss: 0.0232 - accuracy: 0.9927 - val_loss: 0.9700 - val_accuracy: 0.6791\n",
      "Epoch 8/10\n",
      "533/533 - 123s - loss: 0.0143 - accuracy: 0.9951 - val_loss: 3.2416 - val_accuracy: 0.4898\n",
      "Epoch 9/10\n",
      "533/533 - 123s - loss: 0.0448 - accuracy: 0.9859 - val_loss: 0.0285 - val_accuracy: 0.9932\n",
      "Epoch 10/10\n",
      "533/533 - 123s - loss: 0.0908 - accuracy: 0.9679 - val_loss: 0.0374 - val_accuracy: 0.9944\n",
      "sub_dataset:2\n",
      "Epoch 1/10\n",
      "533/533 - 123s - loss: 0.0259 - accuracy: 0.9906 - val_loss: 0.4254 - val_accuracy: 0.8203\n",
      "Epoch 2/10\n",
      "533/533 - 123s - loss: 0.0491 - accuracy: 0.9846 - val_loss: 3.3549 - val_accuracy: 0.5045\n",
      "Epoch 3/10\n",
      "533/533 - 123s - loss: 0.0274 - accuracy: 0.9916 - val_loss: 0.2022 - val_accuracy: 0.9068\n",
      "Epoch 4/10\n",
      "533/533 - 123s - loss: 0.0517 - accuracy: 0.9841 - val_loss: 0.1829 - val_accuracy: 0.9226\n",
      "Epoch 5/10\n",
      "533/533 - 123s - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.0098 - val_accuracy: 0.9977\n",
      "Epoch 6/10\n",
      "533/533 - 123s - loss: 0.0341 - accuracy: 0.9891 - val_loss: 0.0064 - val_accuracy: 0.9994\n",
      "Epoch 7/10\n",
      "533/533 - 123s - loss: 0.0318 - accuracy: 0.9899 - val_loss: 1.5389 - val_accuracy: 0.7333\n",
      "Epoch 8/10\n",
      "533/533 - 123s - loss: 0.0359 - accuracy: 0.9878 - val_loss: 0.5787 - val_accuracy: 0.7802\n",
      "Epoch 9/10\n",
      "533/533 - 123s - loss: 0.0269 - accuracy: 0.9914 - val_loss: 0.0113 - val_accuracy: 0.9983\n",
      "Epoch 10/10\n",
      "533/533 - 123s - loss: 0.0166 - accuracy: 0.9938 - val_loss: 0.0778 - val_accuracy: 0.9723\n",
      "sub_dataset:3\n",
      "Epoch 1/10\n",
      "533/533 - 123s - loss: 0.0328 - accuracy: 0.9884 - val_loss: 0.2066 - val_accuracy: 0.9271\n",
      "Epoch 2/10\n",
      "533/533 - 123s - loss: 0.0391 - accuracy: 0.9872 - val_loss: 0.0183 - val_accuracy: 0.9955\n",
      "Epoch 3/10\n",
      "533/533 - 123s - loss: 0.0254 - accuracy: 0.9927 - val_loss: 0.0267 - val_accuracy: 0.9932\n",
      "Epoch 4/10\n",
      "533/533 - 123s - loss: 0.0307 - accuracy: 0.9901 - val_loss: 0.0167 - val_accuracy: 0.9955\n",
      "Epoch 5/10\n",
      "533/533 - 123s - loss: 0.1893 - accuracy: 0.9311 - val_loss: 0.2967 - val_accuracy: 0.8831\n",
      "Epoch 6/10\n",
      "533/533 - 122s - loss: 0.1283 - accuracy: 0.9522 - val_loss: 0.1578 - val_accuracy: 0.9435\n",
      "Epoch 7/10\n",
      "533/533 - 123s - loss: 0.1400 - accuracy: 0.9503 - val_loss: 0.3691 - val_accuracy: 0.8249\n",
      "Epoch 8/10\n",
      "533/533 - 122s - loss: 0.0957 - accuracy: 0.9638 - val_loss: 5.5759 - val_accuracy: 0.4644\n",
      "Epoch 9/10\n",
      "533/533 - 123s - loss: 0.0810 - accuracy: 0.9707 - val_loss: 0.2680 - val_accuracy: 0.8864\n",
      "Epoch 10/10\n",
      "533/533 - 122s - loss: 0.1043 - accuracy: 0.9593 - val_loss: 0.2403 - val_accuracy: 0.8859\n",
      "sub_dataset:4\n",
      "Epoch 1/10\n",
      "533/533 - 123s - loss: 0.1056 - accuracy: 0.9593 - val_loss: 0.0317 - val_accuracy: 0.9904\n",
      "Epoch 2/10\n",
      "533/533 - 122s - loss: 0.0681 - accuracy: 0.9775 - val_loss: 0.0334 - val_accuracy: 0.9881\n",
      "Epoch 3/10\n",
      "533/533 - 123s - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.1106 - val_accuracy: 0.9503\n",
      "Epoch 4/10\n",
      "533/533 - 123s - loss: 0.0902 - accuracy: 0.9679 - val_loss: 0.0487 - val_accuracy: 0.9836\n",
      "Epoch 5/10\n",
      "533/533 - 123s - loss: 0.0641 - accuracy: 0.9775 - val_loss: 0.0198 - val_accuracy: 0.9944\n",
      "Epoch 6/10\n",
      "533/533 - 122s - loss: 0.0523 - accuracy: 0.9827 - val_loss: 0.0954 - val_accuracy: 0.9588\n",
      "Epoch 7/10\n",
      "533/533 - 122s - loss: 0.1268 - accuracy: 0.9529 - val_loss: 0.0919 - val_accuracy: 0.9712\n",
      "Epoch 8/10\n",
      "533/533 - 122s - loss: 0.0996 - accuracy: 0.9636 - val_loss: 6.3341 - val_accuracy: 0.5136\n",
      "Epoch 9/10\n",
      "533/533 - 122s - loss: 0.1347 - accuracy: 0.9492 - val_loss: 0.0435 - val_accuracy: 0.9814\n",
      "Epoch 10/10\n",
      "533/533 - 123s - loss: 0.0718 - accuracy: 0.9732 - val_loss: 0.0840 - val_accuracy: 0.9701\n",
      "sub_dataset:5\n",
      "Epoch 1/10\n",
      "533/533 - 123s - loss: 0.0531 - accuracy: 0.9816 - val_loss: 0.0108 - val_accuracy: 0.9966\n",
      "Epoch 2/10\n",
      "533/533 - 122s - loss: 0.0848 - accuracy: 0.9698 - val_loss: 0.1124 - val_accuracy: 0.9582\n",
      "Epoch 3/10\n",
      "533/533 - 122s - loss: 0.0648 - accuracy: 0.9769 - val_loss: 0.0256 - val_accuracy: 0.9932\n",
      "Epoch 4/10\n",
      "533/533 - 123s - loss: 0.1064 - accuracy: 0.9589 - val_loss: 0.8426 - val_accuracy: 0.6876\n",
      "Epoch 5/10\n",
      "533/533 - 123s - loss: 0.1098 - accuracy: 0.9593 - val_loss: 0.0513 - val_accuracy: 0.9814\n",
      "Epoch 6/10\n",
      "533/533 - 123s - loss: 0.0581 - accuracy: 0.9795 - val_loss: 0.0221 - val_accuracy: 0.9927\n",
      "Epoch 7/10\n",
      "533/533 - 123s - loss: 0.0779 - accuracy: 0.9713 - val_loss: 0.1966 - val_accuracy: 0.9316\n",
      "Epoch 8/10\n",
      "533/533 - 123s - loss: 0.0581 - accuracy: 0.9790 - val_loss: 0.2647 - val_accuracy: 0.8915\n",
      "Epoch 9/10\n",
      "533/533 - 123s - loss: 0.0701 - accuracy: 0.9747 - val_loss: 0.0100 - val_accuracy: 0.9977\n",
      "Epoch 10/10\n",
      "533/533 - 123s - loss: 0.0451 - accuracy: 0.9848 - val_loss: 0.1019 - val_accuracy: 0.9701\n",
      "sub_dataset:6\n",
      "Epoch 1/10\n",
      "533/533 - 123s - loss: 0.0512 - accuracy: 0.9814 - val_loss: 0.0308 - val_accuracy: 0.9876\n",
      "Epoch 2/10\n",
      "533/533 - 123s - loss: 0.0486 - accuracy: 0.9839 - val_loss: 0.2879 - val_accuracy: 0.8695\n",
      "Epoch 3/10\n",
      "533/533 - 123s - loss: 0.0415 - accuracy: 0.9852 - val_loss: 0.0195 - val_accuracy: 0.9960\n",
      "Epoch 4/10\n",
      "533/533 - 123s - loss: 0.1193 - accuracy: 0.9582 - val_loss: 2.4472 - val_accuracy: 0.6073\n",
      "Epoch 5/10\n",
      "533/533 - 123s - loss: 0.0667 - accuracy: 0.9737 - val_loss: 0.0385 - val_accuracy: 0.9910\n",
      "Epoch 6/10\n",
      "533/533 - 123s - loss: 0.0464 - accuracy: 0.9856 - val_loss: 0.3241 - val_accuracy: 0.8853\n",
      "Epoch 7/10\n",
      "533/533 - 123s - loss: 0.0385 - accuracy: 0.9884 - val_loss: 0.0113 - val_accuracy: 0.9972\n",
      "Epoch 8/10\n",
      "533/533 - 123s - loss: 0.0584 - accuracy: 0.9786 - val_loss: 0.0220 - val_accuracy: 0.9915\n",
      "Epoch 9/10\n",
      "533/533 - 123s - loss: 0.0598 - accuracy: 0.9792 - val_loss: 0.0119 - val_accuracy: 0.9960\n",
      "Epoch 10/10\n",
      "533/533 - 123s - loss: 0.0337 - accuracy: 0.9872 - val_loss: 0.0124 - val_accuracy: 0.9966\n",
      "sub_dataset:7\n",
      "Epoch 1/10\n",
      "533/533 - 123s - loss: 0.0447 - accuracy: 0.9835 - val_loss: 0.2173 - val_accuracy: 0.9232\n",
      "Epoch 2/10\n",
      "533/533 - 123s - loss: 0.0690 - accuracy: 0.9764 - val_loss: 0.0256 - val_accuracy: 0.9915\n",
      "Epoch 3/10\n",
      "533/533 - 123s - loss: 0.0467 - accuracy: 0.9820 - val_loss: 0.2685 - val_accuracy: 0.8859\n",
      "Epoch 4/10\n",
      "533/533 - 123s - loss: 0.0456 - accuracy: 0.9857 - val_loss: 1.4465 - val_accuracy: 0.7090\n",
      "Epoch 5/10\n",
      "533/533 - 122s - loss: 0.0569 - accuracy: 0.9795 - val_loss: 0.1503 - val_accuracy: 0.9525\n",
      "Epoch 6/10\n",
      "533/533 - 123s - loss: 0.1118 - accuracy: 0.9565 - val_loss: 0.0850 - val_accuracy: 0.9672\n",
      "Epoch 7/10\n",
      "533/533 - 123s - loss: 0.1228 - accuracy: 0.9544 - val_loss: 0.3583 - val_accuracy: 0.8695\n",
      "Epoch 8/10\n",
      "533/533 - 123s - loss: 0.1154 - accuracy: 0.9589 - val_loss: 0.1036 - val_accuracy: 0.9605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "533/533 - 122s - loss: 0.0722 - accuracy: 0.9743 - val_loss: 0.0373 - val_accuracy: 0.9881\n",
      "Epoch 10/10\n",
      "533/533 - 122s - loss: 0.0713 - accuracy: 0.9741 - val_loss: 0.0931 - val_accuracy: 0.9593\n",
      "sub_dataset:8\n",
      "Epoch 1/10\n",
      "533/533 - 123s - loss: 0.0596 - accuracy: 0.9786 - val_loss: 3.9115 - val_accuracy: 0.4791\n",
      "Epoch 2/10\n",
      "533/533 - 122s - loss: 0.0725 - accuracy: 0.9745 - val_loss: 0.0206 - val_accuracy: 0.9915\n",
      "Epoch 3/10\n",
      "533/533 - 123s - loss: 0.0410 - accuracy: 0.9854 - val_loss: 0.0654 - val_accuracy: 0.9797\n",
      "Epoch 4/10\n",
      "533/533 - 123s - loss: 0.0309 - accuracy: 0.9893 - val_loss: 0.0189 - val_accuracy: 0.9944\n",
      "Epoch 5/10\n",
      "533/533 - 122s - loss: 0.0451 - accuracy: 0.9846 - val_loss: 0.0488 - val_accuracy: 0.9808\n",
      "Epoch 6/10\n",
      "533/533 - 123s - loss: 0.1191 - accuracy: 0.9552 - val_loss: 0.1157 - val_accuracy: 0.9565\n",
      "Epoch 7/10\n",
      "533/533 - 122s - loss: 0.0998 - accuracy: 0.9634 - val_loss: 0.0896 - val_accuracy: 0.9684\n",
      "Epoch 8/10\n",
      "533/533 - 122s - loss: 0.0684 - accuracy: 0.9758 - val_loss: 0.0423 - val_accuracy: 0.9870\n",
      "Epoch 9/10\n",
      "533/533 - 122s - loss: 0.0804 - accuracy: 0.9700 - val_loss: 0.0567 - val_accuracy: 0.9853\n",
      "Epoch 10/10\n",
      "533/533 - 122s - loss: 0.2111 - accuracy: 0.9165 - val_loss: 1.5291 - val_accuracy: 0.5853\n",
      "sub_dataset:9\n",
      "Epoch 1/10\n",
      "533/533 - 123s - loss: 0.1718 - accuracy: 0.9349 - val_loss: 0.6498 - val_accuracy: 0.7588\n",
      "Epoch 2/10\n",
      "533/533 - 122s - loss: 0.1012 - accuracy: 0.9630 - val_loss: 1.6324 - val_accuracy: 0.6249\n",
      "Epoch 3/10\n",
      "533/533 - 122s - loss: 0.1360 - accuracy: 0.9516 - val_loss: 0.2926 - val_accuracy: 0.8480\n",
      "Epoch 4/10\n",
      "533/533 - 123s - loss: 0.0964 - accuracy: 0.9649 - val_loss: 0.0966 - val_accuracy: 0.9588\n",
      "Epoch 5/10\n",
      "533/533 - 122s - loss: 0.0554 - accuracy: 0.9782 - val_loss: 0.1512 - val_accuracy: 0.9356\n",
      "Epoch 6/10\n",
      "533/533 - 122s - loss: 0.0487 - accuracy: 0.9829 - val_loss: 0.1687 - val_accuracy: 0.9294\n",
      "Epoch 7/10\n",
      "533/533 - 122s - loss: 0.0716 - accuracy: 0.9728 - val_loss: 2.7361 - val_accuracy: 0.4989\n",
      "Epoch 8/10\n",
      "533/533 - 122s - loss: 0.0745 - accuracy: 0.9737 - val_loss: 0.0449 - val_accuracy: 0.9853\n",
      "Epoch 9/10\n",
      "533/533 - 122s - loss: 0.0452 - accuracy: 0.9824 - val_loss: 0.1209 - val_accuracy: 0.9492\n",
      "Epoch 10/10\n",
      "533/533 - 122s - loss: 0.0356 - accuracy: 0.9867 - val_loss: 0.2798 - val_accuracy: 0.8723\n"
     ]
    }
   ],
   "source": [
    "results = training_with_small_dataset(train_X, train_Y, val_X, val_Y, BATCH_SIZE, EPOCHS)\n",
    "# results = training_all_at_once(train_X, train_Y, val_X, val_Y, BATCH_SIZE, EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "472eea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(f\"model{MODEL_NUM}_E{EPOCHS}_B{BATCH_SIZE}_T{train_num}_V{val_num}_val-acc{round(results['val_accuracy'][-1],3)}.h5\")\n",
    "\n",
    "model.save(f\"model{MODEL_NUM}_E{EPOCHS}_B{BATCH_SIZE}_T{train_num}_V{val_num}_val-acc{round(np.mean(results['val_accuracy'][:,-1]),3)}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d61fe6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99887007 0.99435025 0.97231638 0.8858757  0.97005647 0.97005647\n",
      " 0.99661016 0.95932204 0.58531076 0.87231636]\n"
     ]
    }
   ],
   "source": [
    "print(results['val_accuracy'][:,-1])\n",
    "np.save(f\"model{MODEL_NUM}_E{EPOCHS}_B{BATCH_SIZE}_T{train_num}_V{val_num}_val-acc{round(np.mean(results['val_accuracy'][:,-1]),3)}_history.npy\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d03ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model2_E15_B10_N2000_val-acc0.564.h5\n",
    "## 트랜스포머없이 resnet 만 사용결과\n",
    "## 데이터 2000개만 (메모리오류)\n",
    "\n",
    "# Epoch 1/15\n",
    "# 200/200 - 65s - loss: 0.3049 - accuracy: 0.8770 - val_loss: 2.3435 - val_accuracy: 0.5420\n",
    "# Epoch 2/15\n",
    "# 200/200 - 43s - loss: 0.1117 - accuracy: 0.9610 - val_loss: 0.3934 - val_accuracy: 0.8285\n",
    "# Epoch 3/15\n",
    "# 200/200 - 43s - loss: 0.0629 - accuracy: 0.9815 - val_loss: 7.9869 - val_accuracy: 0.4540\n",
    "# Epoch 4/15\n",
    "# 200/200 - 43s - loss: 0.0314 - accuracy: 0.9920 - val_loss: 0.1271 - val_accuracy: 0.9540\n",
    "# Epoch 5/15\n",
    "# 200/200 - 43s - loss: 0.0303 - accuracy: 0.9910 - val_loss: 0.0384 - val_accuracy: 0.9905\n",
    "# Epoch 6/15\n",
    "# 200/200 - 43s - loss: 0.0221 - accuracy: 0.9950 - val_loss: 6.5264 - val_accuracy: 0.5030\n",
    "# Epoch 7/15\n",
    "# 200/200 - 43s - loss: 0.0236 - accuracy: 0.9945 - val_loss: 0.4938 - val_accuracy: 0.8365\n",
    "# Epoch 8/15\n",
    "# 200/200 - 43s - loss: 0.0316 - accuracy: 0.9920 - val_loss: 6.4351 - val_accuracy: 0.4750\n",
    "# Epoch 9/15\n",
    "# 200/200 - 43s - loss: 0.0202 - accuracy: 0.9950 - val_loss: 0.2495 - val_accuracy: 0.8995\n",
    "# Epoch 10/15\n",
    "# 200/200 - 43s - loss: 0.0191 - accuracy: 0.9950 - val_loss: 0.4350 - val_accuracy: 0.8280\n",
    "# Epoch 11/15\n",
    "# 200/200 - 43s - loss: 0.0162 - accuracy: 0.9965 - val_loss: 0.0129 - val_accuracy: 0.9965\n",
    "# Epoch 12/15\n",
    "# 200/200 - 43s - loss: 0.0131 - accuracy: 0.9965 - val_loss: 3.5247 - val_accuracy: 0.5040\n",
    "# Epoch 13/15\n",
    "# 200/200 - 43s - loss: 0.0099 - accuracy: 0.9975 - val_loss: 0.5439 - val_accuracy: 0.8000\n",
    "# Epoch 14/15\n",
    "# 200/200 - 43s - loss: 0.0141 - accuracy: 0.9950 - val_loss: 0.2691 - val_accuracy: 0.8865\n",
    "# Epoch 15/15\n",
    "# 200/200 - 43s - loss: 0.0068 - accuracy: 0.9990 - val_loss: 2.0575 - val_accuracy: 0.5645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b900bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model3_E15_B10_N2000_val-acc0.983.h5\n",
    "## CNN + 트랜스포머 결과\n",
    "## 데이터 2000개만 (메모리오류)\n",
    "\n",
    "# Epoch 1/15\n",
    "# 200/200 - 39s - loss: 0.3059 - accuracy: 0.8685 - val_loss: 3.0019 - val_accuracy: 0.4400\n",
    "# Epoch 2/15\n",
    "# 200/200 - 25s - loss: 0.1277 - accuracy: 0.9640 - val_loss: 1.1292 - val_accuracy: 0.6025\n",
    "# Epoch 3/15\n",
    "# 200/200 - 25s - loss: 0.0928 - accuracy: 0.9685 - val_loss: 0.2415 - val_accuracy: 0.8770\n",
    "# Epoch 4/15\n",
    "# 200/200 - 25s - loss: 0.1089 - accuracy: 0.9650 - val_loss: 0.1295 - val_accuracy: 0.9525\n",
    "# Epoch 5/15\n",
    "# 200/200 - 25s - loss: 0.0501 - accuracy: 0.9875 - val_loss: 0.2519 - val_accuracy: 0.8940\n",
    "# Epoch 6/15\n",
    "# 200/200 - 26s - loss: 0.0878 - accuracy: 0.9715 - val_loss: 0.5996 - val_accuracy: 0.7595\n",
    "# Epoch 7/15\n",
    "# 200/200 - 26s - loss: 0.0609 - accuracy: 0.9790 - val_loss: 0.0753 - val_accuracy: 0.9705\n",
    "# Epoch 8/15\n",
    "# 200/200 - 26s - loss: 0.0270 - accuracy: 0.9950 - val_loss: 0.0416 - val_accuracy: 0.9865\n",
    "# Epoch 9/15\n",
    "# 200/200 - 26s - loss: 0.0467 - accuracy: 0.9845 - val_loss: 0.0839 - val_accuracy: 0.9690\n",
    "# Epoch 10/15\n",
    "# 200/200 - 25s - loss: 0.0366 - accuracy: 0.9900 - val_loss: 0.0764 - val_accuracy: 0.9690\n",
    "# Epoch 11/15\n",
    "# 200/200 - 25s - loss: 0.0266 - accuracy: 0.9925 - val_loss: 0.1577 - val_accuracy: 0.9400\n",
    "# Epoch 12/15\n",
    "# 200/200 - 25s - loss: 0.0238 - accuracy: 0.9940 - val_loss: 0.0344 - val_accuracy: 0.9900\n",
    "# Epoch 13/15\n",
    "# 200/200 - 25s - loss: 0.0407 - accuracy: 0.9895 - val_loss: 0.1058 - val_accuracy: 0.9600\n",
    "# Epoch 14/15\n",
    "# 200/200 - 25s - loss: 0.0143 - accuracy: 0.9975 - val_loss: 0.1084 - val_accuracy: 0.9610\n",
    "# Epoch 15/15\n",
    "# 200/200 - 25s - loss: 0.0108 - accuracy: 0.9980 - val_loss: 0.0619 - val_accuracy: 0.9825\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "745836be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model1_E15_B10_N2000_val-acc0.909.h5\n",
    "## CNN 단일결과\n",
    "## 데이터 2000개만 \n",
    "\n",
    "# Epoch 1/15\n",
    "# 200/200 - 31s - loss: 0.3659 - accuracy: 0.8540 - val_loss: 1.9535 - val_accuracy: 0.4545\n",
    "# Epoch 2/15\n",
    "# 200/200 - 17s - loss: 0.1773 - accuracy: 0.9365 - val_loss: 0.9802 - val_accuracy: 0.6025\n",
    "# Epoch 3/15\n",
    "# 200/200 - 17s - loss: 0.1207 - accuracy: 0.9605 - val_loss: 0.5959 - val_accuracy: 0.7880\n",
    "# Epoch 4/15\n",
    "# 200/200 - 17s - loss: 0.1172 - accuracy: 0.9580 - val_loss: 1.2333 - val_accuracy: 0.6810\n",
    "# Epoch 5/15\n",
    "# 200/200 - 17s - loss: 0.0827 - accuracy: 0.9695 - val_loss: 0.0857 - val_accuracy: 0.9720\n",
    "# Epoch 6/15\n",
    "# 200/200 - 17s - loss: 0.0643 - accuracy: 0.9780 - val_loss: 0.1298 - val_accuracy: 0.9460\n",
    "# Epoch 7/15\n",
    "# 200/200 - 17s - loss: 0.0550 - accuracy: 0.9820 - val_loss: 0.0971 - val_accuracy: 0.9640\n",
    "# Epoch 8/15\n",
    "# 200/200 - 17s - loss: 0.0443 - accuracy: 0.9880 - val_loss: 0.0988 - val_accuracy: 0.9570\n",
    "# Epoch 9/15\n",
    "# 200/200 - 17s - loss: 0.0332 - accuracy: 0.9905 - val_loss: 0.5075 - val_accuracy: 0.8320\n",
    "# Epoch 10/15\n",
    "# 200/200 - 17s - loss: 0.0366 - accuracy: 0.9880 - val_loss: 0.2110 - val_accuracy: 0.9310\n",
    "# Epoch 11/15\n",
    "# 200/200 - 17s - loss: 0.0215 - accuracy: 0.9955 - val_loss: 0.1176 - val_accuracy: 0.9510\n",
    "# Epoch 12/15\n",
    "# 200/200 - 17s - loss: 0.0411 - accuracy: 0.9870 - val_loss: 0.1250 - val_accuracy: 0.9545\n",
    "# Epoch 13/15\n",
    "# 200/200 - 17s - loss: 0.0361 - accuracy: 0.9900 - val_loss: 0.3029 - val_accuracy: 0.8920\n",
    "# Epoch 14/15\n",
    "# 200/200 - 17s - loss: 0.0265 - accuracy: 0.9915 - val_loss: 0.2016 - val_accuracy: 0.9325\n",
    "# Epoch 15/15\n",
    "# 200/200 - 17s - loss: 0.0316 - accuracy: 0.9895 - val_loss: 0.2543 - val_accuracy: 0.9085"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
